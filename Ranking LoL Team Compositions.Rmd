---
title: "Ranking League of Legends Team Compositions"
author: "Ethan Shen"
fontsize: 12pt
geometry: "left=1.4cm,right=1.4cm,top=1.4cm,bottom=1.4cm"
output: 
  pdf_document:
     latex_engine: xelatex
     number_sections: true
---

```{r, function checking for installed packages, include=FALSE}
# Validate that all necessary packaged have been downloaded, install otherwise or throw err package DNE
pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,repos = "http://cran.r-project.org", dep=TRUE)
    if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

pkgTest("tidyverse")
pkgTest("gridExtra")
pkgTest("grid")
pkgTest("kableExtra")
pkgTest("rstan")
pkgTest("tidybayes")
pkgTest("pander")
pkgTest("loo")
```

```{r include=FALSE}
library(tidyverse)
library(gridExtra)
library(grid)
library(kableExtra)
library(rstan)
library(tidybayes)
library(pander)
library(loo)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
ggplot2::theme_set(new = theme_bw())
```

```{r helper functions}
`%notin%` <- Negate(`%in%`)
display_ranking_betas = function(contest_df, model_fit, start_row=1, end_row=10) {
  bind_rows(
    tibble(
      comp = contest_df$winner %>% unique() ,
      comp_id = contest_df$winner %>% as.numeric() %>% unique() 
    ),
    tibble(
      comp = contest_df$loser %>% unique() ,
      comp_id = contest_df$loser %>% as.numeric() %>% unique() 
    )
  ) %>% 
    distinct() %>% arrange(comp_id) %>% 
    inner_join(
      tidybayes::gather_draws(model_fit, beta[i]) %>% 
        group_by(i) %>% 
        summarise(mean = mean(.value),
                  lower = quantile(.value, .025),
                  `50%` = quantile(.value, .5),
                  upper = quantile(.value, 0.975)),
      by = c("comp_id"="i")
    ) %>% 
    inner_join(
      win_perc_tbl,
      by = c(
        "comp" = "team_comp"
      )
    )  %>% 
    arrange(desc(mean)) %>% distinct() %>% 
    mutate(lower =  round(lower,3),
           upper =  round(upper,3),
           `95% CI` = paste0("(", lower, ", ", upper, ")") %>% stringr::str_trim(),
           win_perc = win_perc*100,
           mean = round(mean,3),
           rownum = row_number()) %>% 
    select(rownum, comp_id, comp, mean, `95% CI`, win_perc) %>% 
    rename(" " = rownum,
           "Comp ID" = comp_id,
           "Composition" = comp,
           "Strength" = mean, 
           "Win %" = win_perc) %>%
    slice(start_row:end_row) 
}

display_rankings = function(contest_df, model_fit) {
  bind_rows(
    tibble(
      comp = contest_df$winner %>% unique() ,
      comp_id = contest_df$winner %>% as.numeric() %>% unique() 
    ),
    tibble(
      comp = contest_df$loser %>% unique() ,
      comp_id = contest_df$loser %>% as.numeric() %>% unique() 
    )
  ) %>% 
    distinct() %>% arrange(comp_id) %>% 
    inner_join(
      tidybayes::gather_draws(model_fit, ranking[i]) %>% 
        group_by(i) %>% 
        summarise(mean = mean(.value),
                  lower = quantile(.value, .025),
                  `50%` = quantile(.value, .5),
                  upper = quantile(.value, 0.975)),
      by = c("comp_id"="i")
    ) %>% 
    arrange(desc(mean)) %>% 
    inner_join(
      win_perc_tbl,
      by = c(
        "comp" = "team_comp"
      )
    )  %>% 
    arrange(desc(mean)) %>% distinct()
}



plot_posterior_betas = function(model_fit) {
  
  post_mean_rankings <- tidybayes::gather_draws(model_fit, beta[i]) %>% 
    mutate(param = paste0(.variable, "[", i, "]")) %>% 
    group_by(param, i) %>% 
    summarise(mean = mean(.value)) %>% 
    arrange(desc(mean)) 
  
  bottomids.model_fit <- post_mean_rankings %>% tail(5) %>% pull(i)
  topids.model_fit <- post_mean_rankings %>% head(5) %>% pull(i)
  
  tidybayes::gather_draws(model_fit, beta[i]) %>% 
    ungroup() %>% 
    left_join(post_mean_rankings, by = "i") %>% 
    filter(i %in% c(topids.model_fit, bottomids.model_fit)) %>% 
    ggplot() + 
    geom_density(aes(x = .value, color = reorder(as.factor(i), -mean)), alpha = 0.2) +
    geom_vline(aes(xintercept = mean, color=as.factor(i)), alpha = 0.7, size = 0.8) +
    labs(title = "Top and Bottom 5 Ranked Team Compositions",
         subtitle = expression(paste("Ranked by Descending Strength, ", lambda[i])),
         x = expression(paste("Posterior Values of ", lambda[i])),
         y = "Density", 
         color = "Comp ID") + scale_color_manual(values = col.pal) +
    guides(col = guide_legend(ncol=2))
}
```

```{r set colors}
col.pal = c("#a50026","#d73027","#f46d43","#fdae61","#fee090","#e0f3f8","#abd9e9","#74add1","#4575b4","#313695")
```

```{r load data}
final_data <- readRDS(file = "final_data_sub2.Rds")
win_perc_tbl <- final_data %>% 
  group_by(team_comp) %>% 
  mutate(win_count = sum(ifelse(win == "True", 1, 0)) / 5,
         comp_count = n()/5) %>% 
  mutate(win_perc = round(win_count / comp_count, digits = 4)) %>% 
  select(team_comp, win_perc, win_count, comp_count) %>% 
  distinct()

temp_bt_df <- final_data %>% 
  ungroup() %>% 
  mutate(d = row_number()) %>% 
  select(d, win, team_comp) %>% 
  mutate(win = case_when(
    win == "False" ~ "loser", 
    win == "True" ~ "winner"
  )) %>% 
  tidyr::spread(key=win, value = team_comp) 

contest <- bind_cols(
  temp_bt_df %>% select(winner) %>% na.omit(),
  temp_bt_df %>% select(loser) %>% na.omit()
) %>% 
  slice(which(row_number() %% 5 == 1)) 

matches <- contest %>% 
  mutate(winner = factor(winner, 
                         levels = c(contest$winner, contest$loser) %>% unique() %>% sort()),
         loser = factor(loser, 
                        levels = c(contest$winner, contest$loser) %>% unique() %>% sort())
  ) 

BT_predictors <- final_data %>%
  ungroup() %>%
  mutate(win = case_when(
    win == "False" ~ 0,
    win == "True" ~ 1
  ) %>% as.factor(),
  championPoints =  scale(championPoints),
  lastPlayTime = scale(lastPlayTime),
  totalChampion_mastery_score = scale(totalChampion_mastery_score),
  chestGranted = ifelse(chestGranted == TRUE, 1, 0),
  gameId = rownum) %>% 
  select(win, rownum, championPoints, totalChampion_mastery_score, lastPlayTime, team_comp) %>% 
  mutate(team_comp = factor(team_comp,
                            levels = c(as.character(matches$winner), as.character(matches$loser)) %>% unique() %>% sort()),
         championPoints = scale(championPoints) %>% as.numeric(), 
         totalChampion_mastery_score = scale(totalChampion_mastery_score) %>% as.numeric(), 
         lastPlayTime = scale(lastPlayTime) %>% as.numeric())
```

```{r load data pt2}
matches_cols <- readRDS("matches.Rds")
matches_fit <- matches_cols %>% 
  select(-winner_num, -loser_num) %>% 
  mutate(outcome = as.numeric(outcome)) %>% 
  bind_cols(
    BT_predictors %>% 
      group_by(rownum, win) %>% 
      summarise(sum_ChampPoints = sum(championPoints, na.rm = TRUE),
                sum_totalChampion_mastery_score = sum(totalChampion_mastery_score, na.rm = TRUE),
                sum_lastPlayTime = sum(lastPlayTime, na.rm = TRUE)) %>% 
      ungroup() %>% 
      group_by(rownum) %>% 
      mutate(diff_ChampPoints = sum_ChampPoints - lag(sum_ChampPoints),
             diff_totalChampion_mastery_score = sum_totalChampion_mastery_score - lag(sum_totalChampion_mastery_score),
             diff_lastPlayTime = sum_lastPlayTime - lag(sum_lastPlayTime)) %>% 
      na.omit() %>% 
      ungroup() %>% 
      select(diff_ChampPoints:diff_lastPlayTime)
  )
```

```{r train test set}
set.seed(20200922)
bound <- floor((nrow(matches_fit)/4)*3)         #define % of training and test set

matches_fit <- matches_fit[sample(nrow(matches_fit)), ]           #sample rows 
matches_fit.train <- matches_fit[1:bound, ]              #get training set
matches_fit.test <- matches_fit[(bound+1):nrow(matches_fit), ]  
```

```{r EDA}
matches_cols_for_EDA <-  matches_cols %>% 
  bind_cols(
    BT_predictors %>% 
      group_by(rownum, win) %>% 
      summarise(sum_ChampPoints = sum(championPoints, na.rm = TRUE),
                sum_totalChampion_mastery_score = sum(totalChampion_mastery_score, na.rm = TRUE),
                sum_lastPlayTime = sum(lastPlayTime, na.rm = TRUE)) %>% 
      ungroup() %>% 
      group_by(rownum) %>% 
      mutate(diff_ChampPoints = sum_ChampPoints - lag(sum_ChampPoints),
             diff_totalChampion_mastery_score = sum_totalChampion_mastery_score - lag(sum_totalChampion_mastery_score),
             diff_lastPlayTime = sum_lastPlayTime - lag(sum_lastPlayTime)) %>% 
      na.omit() %>% 
      ungroup() %>% 
      select(diff_ChampPoints:diff_lastPlayTime)
  ) 

p1 <- win_perc_tbl %>% 
  ggplot() + 
  geom_histogram(aes(x = win_perc, y = ..density..), binwidth=0.025, colour="black", fill="white") + 
  geom_density(aes(x = win_perc), alpha=.1, fill="#FF6666") + 
  labs(title = "Distribution of Win Percentages",
       x = "Win Percentage",
       y = "Density",
       caption = "Figure 1") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 16),
        plot.title = element_text(size = 18)) 

p2 <- matches_cols_for_EDA %>% 
  ggplot(aes(x = diff_ChampPoints, y = diff_totalChampion_mastery_score, color = as.factor(winner_num))) + 
  geom_point(alpha = 0.15) + 
  geom_line(stat="smooth", method = "lm", formula = y ~ x, alpha = 0.9, se=FALSE) + 
  labs(title = expression(paste(Delta[k], "MS vs. ", Delta[k], "CP")),
       x = expression(paste(Delta[k], "CP")),
       y = expression(paste(Delta[k], "MS")),
       caption = "Figure 2") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 16),
        plot.title = element_text(size = 18))  + 
  theme(legend.position = "none") 
```

```{r}
generate_ranking_model_text = function(priors) {
  data_params = "
  data {
  int<lower = 0> N_games;                    
  int<lower = 0> N_comps;  
  int<lower = 0> N_predictors;
  int<lower = 0, upper = 1> y[N_games]; 
  matrix[N_games, N_comps+N_predictors] X;
  }
  parameters {
   vector[N_comps+N_predictors] beta;
   real<lower = 0> sigma_comp;       
  }
  transformed parameters {
    vector[N_games] y_pred = X * beta;
  }
  model {
    for (i in 1:N_games) {
      y[i] ~ bernoulli_logit((X[i,] * beta));
    }
  
    for(i in 1:N_comps){
       beta[i] ~ normal(0,sigma_comp);
    }
  "
  
  generate_rank = "
  generated quantities {
    int<lower=1, upper=N_comps+N_predictors> ranking[N_comps+N_predictors];       // rank of player ability
    {
      int ranked_index[N_comps+N_predictors] = sort_indices_asc(beta);
      for (k in 1:N_comps+N_predictors)
        ranking[ranked_index[k]] = k;
    }
    for (n in 1:N_comps+N_predictors) {
      log_lik[n] = bernoulli_logit_lpmf(y[n] | X[n] * beta);
    }
  }
  "
  
  model_text <- paste(data_params, priors, generate_rank)
  return(model_text)
}
```

```{r stan gamma-1-1 with interactions final model }
gamma_1_1_beta_0_10.prior_interactions = 
  "
  for (j in (N_comps + 1):(N_comps + N_predictors)) {
     beta[j] ~ normal(0,10);
  }

  sigma_comp ~ gamma(1,1); 
  
}"

model_file_ga_1_1_interactions <- generate_ranking_model_text(gamma_1_1_beta_0_10.prior_interactions)
if (file.exists("rds/final_team_rankings_interactions_with_loglik_v2.Rds")) { 
  ga_1_1_interactions <- readRDS("rds/final_team_rankings_interactions_with_loglik_v2.Rds") 
} else {
  ga_1_1_interactions <- rstan::stan(
    model_code = model_file_ga_1_1_interactions, 
    data = list(
      y = matches_fit %>%  pull(outcome) %>% as.numeric(),
      N_comps = 382,
      N_predictors = 4,
      N_games = nrow(matches_fit), # matches
      X = matches_fit %>% select(-outcome)  %>% as.matrix() 
    ),
    iter = 2500,
    warmup = 500,
    thin = 4,
    chains = 2
  )
  
  saveRDS(ga_1_1_interactions, file = "rds/final_team_rankings_interactions_with_loglik_v2.Rds")
}
```

```{r stan gamma-0.1-1 with interactions}
gamma_0.1_1_beta_0_10.prior_interactions = 
  "
  for (j in (N_comps + 1):(N_comps + N_predictors)) {
     beta[j] ~ normal(0,10);
  }

  sigma_comp ~ gamma(0.1,1); 
  
}"

model_file_ga_0.1_1_interactions <- generate_ranking_model_text(gamma_0.1_1_beta_0_10.prior_interactions)
if (file.exists("rds/final_team_rankings_interactions_ga_0.1_1.Rds")) { 
  ga_0.1_1_interactions <- readRDS("rds/final_team_rankings_interactions_ga_0.1_1.Rds") 
} else {
  ga_0.1_1_interactions <- rstan::stan(
    model_code = model_file_ga_0.1_1_interactions, 
    data = list(
      y = matches_fit %>%  pull(outcome) %>% as.numeric(),
      N_comps = 382,
      N_predictors = 4,
      N_games = nrow(matches_fit), # matches
      X = matches_fit %>% select(-outcome)  %>% as.matrix() 
    ),
    iter = 2500,
    warmup = 500,
    thin = 4,
    chains = 2
  )
  saveRDS(ga_0.1_1_interactions, file = "rds/final_team_rankings_interactions_ga_0.1_1.Rds")
}
```


```{r stan gamma-2-1 with interactions}
gamma_2_1_beta_0_10.prior_interactions = 
  "
  for (j in (N_comps + 1):(N_comps + N_predictors)) {
     beta[j] ~ normal(0,10);
  }

  sigma_comp ~ gamma(2,1); 
  
}"

model_file_ga_2_1_interactions <- generate_ranking_model_text(gamma_2_1_beta_0_10.prior_interactions)
if (file.exists("rds/final_team_rankings_interactions_ga_2_1.Rds")) { 
  ga_2_1_interactions <- readRDS("rds/final_team_rankings_interactions_ga_2_1.Rds") 
} else {
  ga_2_1_interactions <- rstan::stan(
    model_code = model_file_ga_2_1_interactions, 
    data = list(
      y = matches_fit %>%  pull(outcome) %>% as.numeric(),
      N_comps = 382,
      N_predictors = 4,
      N_games = nrow(matches_fit), # matches
      X = matches_fit %>% select(-outcome)  %>% as.matrix() 
    ),
    iter = 2500,
    warmup = 500,
    thin = 4,
    chains = 2
  )
  saveRDS(ga_2_1_interactions, file = "rds/final_team_rankings_interactions_ga_2_1.Rds")
}
```

```{r stan  gamma-1-1 no interactions}
gamma_1_1_beta_0_10.prior = 
  "
  for (j in (N_comps + 1):(N_comps + N_predictors)) {
     beta[j] ~ normal(0,10);
  }

  sigma_comp ~ gamma(1,1); 
  
}"

model_file_ga_1_1 <- generate_ranking_model_text(gamma_1_1_beta_0_10.prior)
if (file.exists("rds/final_team_rankings1.Rds")) { 
  ga_1_1 <- readRDS("rds/final_team_rankings1.Rds") 
} else {
  ga_1_1 <- rstan::stan(
    model_code = model_file_ga_1_1, 
    data = list(
      y = matches_fit %>%  pull(outcome) %>% as.numeric(),
      N_comps = 382,
      N_predictors = 3,
      N_games = nrow(matches_fit), # matches
      X = matches_fit %>% select(-outcome)  %>% as.matrix() 
    ),
    iter = 2500,
    warmup = 500,
    thin = 4,
    chains = 2
  )
  
  saveRDS(ga_1_1, file = "rds/final_team_rankings1.Rds")
}
```

```{r}
#ADD ANOTHER ONE FOR GAMMA(2,1)
```


# Introduction 

League of Legends (LoL) is a multiplayer online battle arena game where two teams, typically composed of 5 players each, attempt to destroy the enemy team’s Nexus, a structure located in the heart of the base and protected by other defensive towers. Players pick from over 140 characters (known as “champions”). Each champion is assigned one of seven classes^[[a]][Terminology]^, which are groups of champions with similar styles. For example, tanks excel in soaking up damage from the enemy team more than being significant damage threats, while slayers are fragile but damage focused champions that look to swiftly take down an enemy.

One important aspect of the game is the team’s champion composition. In general, a better team composition consists of champions of different classes, instead of a team of five tanks or five slayers. Professional teams consist of coaches and analysts who help with the development of team compositions; this process requires intense preparation and a detailed analysis of the opposing team. Often times, professional games are swayed heavily in favor for one team simply because their team composition synergizes better or has more teamfighting advantages than that of the enemy team.

However, in a normal ranked solo-queue^[[b]][Terminology]^ game, teams generally consist of random players with limited in-game communication. Without reliable methods to discuss team compositions^[Team composition in this paper refers to the classes associated with the 5 champions on a team, rather than the names of the champions themselves.], solo-queue players, like myself, may benefit from a team composition ranking system, where a higher ranking indicates that forming a specific team composition may result in a higher chance of winning. This ranking system will be modeled using the Bradley-Terry model [[1]][References]. Although the Bradley-Terry model is generally used to predict the outcome of a paired comparison, the pairwise rankings can be combined to derive a full ranking. For computation purposes, I will reduce the Bradley-Terry model into a hierarchical logistic regression model and fit it in the Bayesian framework. 

The project examines the following questions: 1) How can solo-queue players determine the best classes of champions to play to form a strong team composition? and 2) Which player-level statistics are the most significant features when determining a team composition's overall rank? 

# Literature Review 

The Bradley-Terry model [[1]][References] has numerous applications. In the Bayesian setting, a generalized version that allows for ties in pairwise comparison was used to rate chess players [[2]][References], and a hierarchical version was used to rank MLB teams [[3]][References]. The model has also been applied to LoL data but was used to predict match results [[4]][References], rather than to provide team composition rankings.

# Data

The original data includes 108,829 games in Korean solo-queue played in Master tier or higher played during Seasons 8-10, which were from 2018-2020. I downloaded the data from Kaggle [[5]][References], which was scraped from Riot Games using their API [[6]][References]. Each game contains information about each of the 10 players and numeric attributes about each player’s performance. However, there are many inconsistencies with the players' recorded roles^[[c]][Terminology]^ in original dataset, which are discussed further in the Appendix in Section 8.3. After removing these inconsistencies, there were 31,909 games and 382 unique team compositions.  

```{r}
vars <- linebreak(c("win", "(N = 31,909)", "championPoints (CP)**", "(N = 319,090)", "totalChampionMasteryScore (MS)**", "(N = 319,090)", "lastPlayTime**", "(N = 319,090)"))
var_descriptions <- linebreak(c(
  "Outcome of a Game", "",
  "Amount of Experience a Player","has with a Champion",
  "Total Sum of a Player's", "championPoints",
  "Last Time a Player", "Played a Champion"
))

var_values <- c(
  rep("0 (No), 1 (Yes)",2),
  rep("-0.48 to 13.59",2),
  rep("-1.87 to 3.94",2),
  rep("-12.68 to 0.6",2)
)

tibble(
  vars = vars,
  var_descriptions = var_descriptions,
  var_values = var_values
) %>% 
  kableExtra::kable(caption = "Description of Raw Variables (\\*** Indicates Variable was Mean-Centered and Standardized)", 
        col.names = c("Variable", "Description", "Values"),
        booktabs = TRUE, align = "lcc") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"))  %>% 
  kableExtra::pack_rows("Response", 1, 2) %>% 
  kableExtra::pack_rows("Predictors", 3,8) %>% 
  kableExtra::collapse_rows(columns = c(1, 3), latex_hline = "major") 
```

For each game, there are player-level variables for each of the 10 players, for a total of 319,010 observations per variable. The three player-level variables were chosen based on domain-driven knowledge; they are displayed in Table 1. *championPoints* are accrued for each champion that a player has played, so a higher value indicates that a player has more experience with a specific champion. Each player will thus have a *championPoints* value for each champion they haved played, so *totalChampionMasteryScore* is the sum of a player's *championPoints* values. *lastPlayTime* is a timestamp that indicates the last time a player played a specific champion. *championPoints*, *totalChampionMasteryScore* and *lastPlayTime* were all mean-centered and standardized. 

```{r}
vars_transformed <- linebreak(c("$\\Delta_{ijk}$CP", "(N = 31,909)", "$\\Delta_{ijk}$MS", "(N = 31,909)", "$\\Delta_{ijk}$lastPlayTime", "(N = 31,909)"))

var_descriptions_transformed <- linebreak(c(
  "Difference between the aggregate champion points for", "the 5 players on team $i$ and that of team $j$ in game $k$",
  "Difference between the aggregate mastery score for", "the 5 players on team $i$ and that of team $j$ in game $k$",
  "Difference between the aggregate last time played for", "the 5 players on team $i$ and that of team $j$ in game $k$"
))

var_values_transformed <- c(
  rep("-15.42 to 16.92",2),
  rep("-9.29 to 11.89",2),
  rep("-21.9 to 21.52",2) 
)

tibble(
  vars = vars_transformed,
  var_descrip = var_descriptions_transformed,
  var_values = var_values_transformed
) %>% 
  kable(escape = F, caption = "Description of Transformed Variables ", 
        col.names = c("Variable", "Description", "Values"),
        booktabs = TRUE, align = "lc") %>% 
  kable_styling(latex_options = c("hold_position"), font_size = 11)  %>% 
  kableExtra::pack_rows("Predictors", 1,6) %>% 
  kableExtra::collapse_rows(columns = c(3), latex_hline = "major") 
```

These three player-level variables had to be transformed before they can be incorporated in the model. In each game $k \in (1, \ 2, \ \dots , \ 31909)$, $\Delta_{ijk} CP$ is the difference between the aggregate champion points for the 5 players on team $i$ and that of team $j$. This is done for both $\Delta_{ijk} MS$ and $\Delta_{ijk} lastPlayTime$ as well. The values for these transformed variables are shown in Table 2.  

## EDA 

Figure 1 displays the distribution of win percentages across the different team compositions. Barring team compositions that have a 0% or 100% win rate, the win percentages are approximately normally distributed, with most teams having a win percentage of around 50%. The team compositions that have a 0% or 100% win rate are recorded either once or twice, which indicate that the rankings for these teams may have more variability and is one limitation with the dataset. 

Figure 2 displays the relationship between $\Delta_{ijk}CP$ and $\Delta_{ijk}MS$ for the  382 different team compositions. The figure shows that for certain team compositions, there is a direct relationship between $\Delta_{ijk}CP$ and $\Delta_{ijk}MS$  (indicated by lines with a positive slope), and an inverse relationship for other team compositions (indicated by lines with a negative slope). This supports evidence for an interaction term between these two variables in the model. 

```{r fig.width = 14, fig.height=4.5, fig.align = 'center'}
gridExtra::grid.arrange(p1,p2,ncol=2)
```

# Methodology 

For two team compositions $i$ and $j$, Bradley and Terry [[1]][References] suggested the following model:

$$
\begin{aligned}
Pr(i \ \text{beats}\  j) = \text{logit}^{-1} (\lambda_i - \lambda_j&), & (1) \\
\text{logit}[Pr(i \ \text{beats}\  j)] =  \lambda_i - \lambda_j, && (2)
\end{aligned}
$$
where $\text{logit}^{-1}(\nu) = \frac{exp(\nu)}{1 + exp(\nu)}$ and $\lambda_i \in \mathbb{R}$ represents a team composition's "strength" - a larger value of $\lambda_i$ indicates a higher ranking. In (2), the Bradley-Terry model is reduced to a logistic regression on pairs of team compositions. Since the pairwise rankings can be combined, an overall ranking of team compositions $i \in (1 \dots 382)$ can be determined with a logistic regression model. In addition, this methodology allows for the incorporation of player-level effects. Thus, I fit a hierarchical Bayesian logistic regression model to determine the ranking for all 382 team compositions. The Bayesian framework was selected since it may provide better convergence of estimates and more realistic rankings for the team compositions with 0% or 100% win rate - a frequentist logistic model would rank the team compositions by descending win rate. 

$$
\begin{aligned}
Y_{ijk} \sim Bernoulli(\pi_{ijk}) \\
log\left(\frac{\pi_{ijk}}{1-\pi_{ijk}}\right) = \lambda_i - \lambda_j  + \beta_1*\Delta_{ijk} C&P + \beta_2*\Delta_{ijk} MS \ +\\  
\beta_3*\Delta_{ijk} lastPlayTime  +   \beta_4* (\Delta_{ijk} &CP * \Delta_{ijk} MS)& (3)\\\\
\lambda_i \sim N(0,\sigma^2_{comp}), \ \sigma^2_{comp} \sim G&a(1,1) \\
\beta_1, \ \beta_2, \  \beta_3, \  \beta_4 \sim  N(0,1&0) \\
\end{aligned}
$$

The final model is shown in (3). $Y_{ijk}$ is the outcome of the game when team composition $i$ plays team composition $j$ in game $k \in (1,\ 2, \ \dots ,\ 31,909)$, where $Y_{ijk} = 1$ indicates that team $i$ won. $\pi_{ijk}$ is the probability that $i$ beats $j$ in game $k$. $\lambda_i$ and $\lambda_j$ represent the strength of teams $i$ and $j$ - as a reminder, a higher $\lambda$ value indicates a higher ranking. Each $\lambda_i$ is sampled from a normal distribution centered at 0, with variance $\sigma^2_{comp}$ - this allows for the implementation of a hierarchical component to account for differences among each team composition. In addition, the three player-level variables, and an interaction, are included in the model since a team's overall strength or ranking can be thought of as "the sum of its parts."  Uninformative, flat priors are placed on the $\beta$ coefficients, since there is no prior knowledge on what these coefficients should be. 

Variable selection was performed by trying different combinations of interaction terms - the three player-level variables were always kept in the model. Insignificant interaction terms were then removed. Posterior predictive checks (simulating game outcomes) were also analyzed for each model. The final model (3) produced the best simulated game outcomes, although all 6 models that I tested produced similar estimates and simulated game outcomes. 

# Results 

## Ranking System 

```{r display rankings}
display_ranking_betas(matches, ga_1_1_interactions, 1, 10) %>% 
  kableExtra::kable(caption = "Top 10 Team Compositions by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(font_size = 11.5, latex_options = c("hold_position"))
```

Table 3 displays the top 10 team compositions, ranked by descending strength ($\lambda_i$) - a full ranking is in the Appendix. From the $\lambda_i$ values of these teams, the best team, composition 89, is relatively clear. Its strength value of 0.139 is slightly higher than that of any other top 10 team. However, for these 10 teams, it can be hard to differentiate between the strengths between these teams - these values are relatively small and close to each other. Table 4 displays the weakest 10 team compositions. Similarly, their strength values are also relatively similar, with the exception of the last place team, composition 215,  whos strength value is much lower than that of the other bottom tier teams. 

```{r display rankings pt2}
display_ranking_betas(matches, ga_1_1_interactions, 373, 382) %>% 
  kableExtra::kable(caption = "Bottom 10 Team Compositions by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(font_size = 11.5, latex_options = c("hold_position"))
```

Ultimately, it is hard to distinguish a team composition that is much stronger than the other ones, since the strength values are relatively close across all 382 team compositions. This is highlighted in Figure 3, where the posterior densities of $\lambda_i$ are plotted for the top 5 and bottom 5 teams. We see that the posterior distributions largely overlap. This is mainly because most teams have a win rate of around 50%. Although the model includes a hierarchical component and incorporates player-level statistics, it can struggle to extract significant differences between these team compositions since so many of them have similar win rates. 

```{r fig.align='center', fig.height = 4}
plot_posterior_betas(ga_1_1_interactions) + 
  labs(caption = "Figure 3") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 12)) 
```

In general, a strong team will have a balance between offensive and defensive champions. There are a total of 268 team compositions that contain at least one defensive champion (Controller or Tank). The top 20 ranked team compositions all have balanced compositions - that is, all have at least one defensive champion. These defensive champions give a team composition more viability during the game - during team fights, Controllers can heal or strengthen teammates while Tanks can soak up damage from the enemy team to protect its team. 

However, not all balanced compositions are ranked highly. There are 2 teams in the bottom 10 that also have at least one defensive champion (compositions 59 and 261). This could be due to a multitude of reasons. Recall that a team composition's strength is not solely determined by the 5 champion classes or its win rate - the player-level variables also affect a team composition's final ranking. The players on these teams could have been weaker or less experienced than their opponents, which would contribute to these team compositions' low rankings. 

In terms of team compositions with all offensive champions (no Controller or Tank), the highest is ranked 22nd (team composition 219), while 8 of the bottom 10 consist of team compositions with 5 offensive champions. This further suggests that a balanced team composition will perform better than one that is unbalanced.  

## Significant Player-level Predictors 

```{r}
latex_predictors <- c("$\\Delta_{ijk}$CP", "$\\Delta_{ijk}MS$", "$\\Delta_{ijk}lastPlayTime$", "$\\Delta_{ijk}CP$ * $\\Delta_{ijk}MS$")
gather_draws(ga_1_1_interactions, beta[i]) %>% 
  ungroup() %>% 
  filter(i > 382) %>%
  mutate(param = ifelse(is.na(i), .variable, paste0("beta","[", i, "]"))) %>% 
  group_by(param) %>% 
  summarise(mean = mean(.value) %>% round(3),
            exp_mean = mean(.value) %>% exp() %>% round(3),
            lower = quantile(.value, .025) %>% exp() %>% round(3),
            upper = quantile(.value, 0.975) %>% exp() %>% round(3)) %>% 
  mutate(`95% CI` = paste0("(", lower, ", ", upper, ")") %>% stringr::str_trim(),
         param = latex_predictors,
         Predictor = param,
         Estimate = exp_mean,
         `Log Estimate` = mean) %>% 
  select(Predictor, `Log Estimate`, Estimate, `95% CI`) %>% 
  pander(caption = "Coefficients for Player-Level Variables from (3)")
  # kableExtra::kable(caption = "Coefficients for Player-Level Variables from (3)", 
  #                   booktabs = TRUE, align = "c") %>% 
  # kableExtra::kable_styling()

```

Table 5 displays the exponentiated coefficient values for the 3 player-level variables, $\Delta_{ijk}CP$, $\Delta_{ijk}MS$ and $\Delta_{ijk}lastPlayTime$, as well as the interaction between $\Delta_{ijk}CP$ and $\Delta_{ijk}MS$. We see that all coefficients, except for $\Delta_{ijk}lastPlayTime$, are significant features when determining a team composition's overall rank. Thus, for every increase in the difference in total champion points from team $i$ to that of team $j$, we expect the odds of winning the game to multiply by a factor of $e^{0.024 + 0.016 \Delta_{ijk}MS}$. If $\Delta_{ijk}MS$ is held constant, then we expect a multiplicative increase in odds of 1.024 (1.013, 1.035). This result is as expected - if team $i$ consists of players that have more experience playing their 5 champions than the players on team $j$, we expect team $i$ to have a slightly higher chance of winning. 

In addition, for every increase in the difference in total mastery score from team $i$ to that of team $j$, we expect the odds of winning the game to multiply by a factor of $e^{-0.017 + 0.016 \Delta_{ijk}CP}$. If $\Delta_{ijk}CP$ is held constant, then we expect a multiplicative increase in odds of 0.983 (0.971, 0.996). Thus, holding each team's experience on the champions they are playing fixed, increasing team $i$'s  experience on champions they are *not* playing decreases team $i$'s chances of winning the game. This may be surprising result at first. However, recall that *totalChampionMasteryScore* is the sum of a player's *championPoints*. For example, consider hypothetical player X, who has played champions Lee Sin, Zac, and Renekton Player X has played Lee Sin and Zac extensively, but has rarely played Renekton. Player X will have a high *totalChampionMasteryScore* and *championPoints* for Lee Sin and Zac, but a low *championPoints* for Renekton. In game $k$, Player X, who is on team $i$, decides to play Renekton. Assume that because of Player X's experience playing Lee Sin and Zac, team $i$'s aggregate *totalChampionMasteryScore* is much higher than that of their oppponent, team $j$. However, team $i$ may be more likely to lose the game simply because Player X is not an experienced Renekton player. Thus, this result  suggests that someone on team  $i$ is  playing an off-role champion or are over-confident in their abilities on a champion they do not have much experience with. 

These results suggest that more overall experience playing the game (a higher *totalChampionMasteryScore*) is **not** as important as gaining experience playing specific champions (a higher *championPoints*). Thus solo-queue players may have a higher chance of winning when playing champions they have more experience with. 

## Model Diagnostics 

Figures 4 and 5 in the Appendix display traceplots for the top four $\lambda_i$'s (team compositions 89, 60, 264, and 18) and the $\beta$'s.The quantiles for $\hat{R}$ values are shown in Table 6 in the Appendix. The traceplots indicate the chains have mixed well and $\hat{R}$ values close to 1, indicating that our model generated from (3) has sufficiently converged. 

### Sensitivity Analysis 

I display the top 10 team compositions and their estimated strength values ($\lambda_i$) across different prior distributions on $\sigma^2_{comp}$. Across the three priors, we see that the estimated $lambda_i$ values for the top 10 teams are relatively consistent and that the top three teams (89, 60 and 264) are the same. The top 10 teams estimated by the $Ga(0.1, 1)$ prior on $\sigma^2_{comp}$ are the same as those from (3), except it orders the teams ranked 4-10 slightly differently. The top 10 team compositions estimated by the  $Ga(2,1)$ prior on $\sigma^2_{comp}$ includes 8 of the same team compositions estimated by the two previous priors, with the addition of compositions 90 and 297, instead of 8 and 76 in its top 10. Thus, the results are relatively insensitive to prior choice since there are no drastic discrepancies across different priors [[7]][References].

```{r}
t1 <- display_ranking_betas(matches, ga_1_1_interactions, 1, 10) %>% 
  select(" ",  `Comp ID`, Strength)
t3 <- display_ranking_betas(matches, ga_0.1_1_interactions, 1, 10) %>% 
  select(`Comp ID`, Strength)
t4 <- display_ranking_betas(matches, ga_2_1_interactions, 1, 10) %>% 
  select(`Comp ID`, Strength)
bind_cols(t1,t3,t4) %>% 
  kableExtra::kable("latex", col.names = c("", "Comp ID", "Strength", "Comp ID", "Strength", "Comp ID", "Strength"), booktabs = TRUE,
                    caption = "Top 10 Team Compositions by Strength, $\\hat{\\lambda}_i$, from Different Priors on $\\sigma^2_{comp}$") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position")) %>% 
  kableExtra::add_header_above(c(" " = 1, 
                                 "Ga(1, 1)" = 2, 
                                 "Ga(0.1, 1)" = 2, 
                                 "Ga(2, 1)" = 2),
                               escape = F)  
```

## Validation

### Cross Validation

To check the validity of (3), I compute an approximate LOOCV using `PSIS-LOO` [[8]][References]. The printed output in Table 7 shows the estimates $\widehat{elpd}_{loo}$ (expected log predictive density), $\widehat{p}_{loo}$ (effective number of parameters), and $-2\widehat{elpd}_{loo}$ (the LOO information criterion). In addition, the output states that all Pareto $k$ estimates are good ($k$ < 0.5) - the interpretation of $k$ can be found in greater detail in Vehtari, Simpson, Gelman, Yao, and Gabry [[8]][References]. This output indicates that all the estimates for $k$ are fine, thus providing an initial validation for this model. 

```{r loo LOOCV}
#http://mc-stan.org/loo/articles/loo2-with-rstan.html
library(loo)
# Extract pointwise log-likelihood
# using merge_chains=FALSE returns an array, which is easier to 
# use with relative_eff()
log_lik_1 <- extract_log_lik(ga_1_1_interactions, merge_chains = FALSE)
# as of loo v2.0.0 we can optionally provide relative effective sample sizes
# when calling loo, which allows for better estimates of the PSIS effective
# sample sizes and Monte Carlo error
r_eff <- relative_eff(exp(log_lik_1), cores = 2)

# preferably use more than 2 cores (as many cores as possible)
# will use value of 'mc.cores' option if cores is not specified
loo_1 <- loo(log_lik_1, r_eff = r_eff, cores = 2)

loo_1$estimates %>% 
  as.data.frame() %>% 
  mutate(Estimates = c("Expected Log Predictive Density (epld)", 
                     "Effective  Number of Parameters (p)", "LOO Information Criterion (looic)")) %>% 
  mutate_if(is.double,  round, 3) %>% 
  select(Estimates, Estimate, SE) %>% 
  kableExtra::kable(caption = "Using Efficient PSIS-Loo Approximation to Exact LOOCV", 
                    booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position")) %>% 
  kableExtra::footnote(general = "All Pareto k estimates are good (k < 0.5)")
```

### Simulating Game Outcomes 

As another form of validation, I simulate game outcomes using the $\lambda_i$'s and $\beta$'s that were generated from (3) and then compare them to the actual outcomes. To simulate the game outcomes, I first calculate $\pi_{ij}$, the overall probability that team $i$ beats team $j$. From (3), we know that 
$$
\begin{aligned}
log\left(\frac{\pi_{ij}}{1-\pi_{ij}}\right) = \lambda_i - \lambda_j + \sum&_{p=1}^4 \beta_p & (4) \\
\pi_{ij} = \frac{e^{\lambda_i - \lambda_j + \sum_{p=1}^4 \beta_p}}{1 + e^{\lambda_i - \lambda_j + \sum_{p=1}^4 \beta_p}} && (5)
\end{aligned}
$$

Then we take the probability obtained in (5) and sample 100,000 game outcomes from a Bernoulli distribution. 

```{r eval=FALSE}
# most played comps: 
#Controller, Fighter, Fighter, Mage, Marksman - 4214 total games
# Controller, Fighter, Mage, Marksman, Slayer	 - 3420 
# Controller, Fighter, Fighter, Marksman, Slayer - 2490 games
win_perc_tbl 
comps_df %>% 
  filter(`Team Composition` %in% c("Controller, Fighter, Fighter, Mage, Marksman", # 55
                                   "Controller, Fighter, Mage, Marksman, Slayer",# 75
                                   "Controller, Fighter, Fighter, Marksman, Slayer")) # 60
matches %>% 
  group_by(winner, loser) %>% 
  mutate(n = n()) %>% 
  distinct() %>% 
  arrange(desc(n)) 
```

```{r}
comps_df <- BT_predictors %>% select(team_comp) %>% 
  mutate(comp_id = as.numeric(team_comp), 
         "Team Composition" = team_comp, 
         "Comp ID" = comp_id) %>% 
  distinct() %>% arrange(comp_id) %>% 
  select("Team Composition", "Comp ID")

actual_v1 <- matches %>% 
  filter((winner == "Controller, Fighter, Fighter, Mage, Marksman" & loser == "Controller, Fighter, Mage, Marksman, Slayer") | 
           (loser == "Controller, Fighter, Fighter, Mage, Marksman" & winner == "Controller, Fighter, Mage, Marksman, Slayer")) %>% 
  left_join(comps_df, by = c("winner" = "Team Composition")) %>% 
  group_by(winner, loser) %>% 
  mutate(n = n()) %>% 
  distinct() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  mutate(prop_win = n / sum(n))  %>% #55 beats #75  wit hprob 0.4869
  select(-loser) %>% 
  mutate(simulation = "Actual")

beta_sim1 <- 0.03 - 0.081 + 0.024  - 0.017 +  0.002 +  0.016
pi_sim1 <- exp(beta_sim1) / (1 + exp(beta_sim1))
set.seed(20200922)
simulation_v1 <- rbinom(100000, 1, pi_sim1) %>% 
  as_tibble() %>% 
  group_by(value) %>% 
  mutate(n = n()) %>% 
  distinct() %>% 
  mutate(`Comp ID` = ifelse(value == 1, 55, 75),
         winner = ifelse(value == 1, "Controller, Fighter, Fighter, Mage, Marksman", "Controller, Fighter, Mage, Marksman, Slayer"),
         simulation = "Simulated") %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  mutate(prop_win = n / sum(n)) %>% 
  select(winner, `Comp ID`, n, prop_win, simulation)
```

Figure 7 displays the actual and simulated outcomes of games between team composition 55 vs. 75. These two team compositions are the most played: team 55 has been played 4,214 times while team 75 has been played 3,420 times. These two teams also have the most head to head games, 191. Out of these 191 games, team composition 55 won 93, or 48.691%, of its games. The estimated value of $\pi_{(55, 75)}$ is `r pi_sim1 %>% round(3)`. From 100,000 simulated games, team composition 55 won `r  simulation_v1 %>% filter(winner == "Controller, Fighter, Fighter, Mage, Marksman") %>% pull(prop_win)%>% {.*100} %>% round(3)`% of its games. The proportions of the actual and simulated game outcomes are very similar - this indicates that our model performs well for matchups that have occured multiple times in the dataset. 


```{r sim-games-EDA1, fig.height=4, fig.align = 'center'}
bind_rows(
  actual_v1,
  simulation_v1
) %>% 
  ggplot(aes(x = simulation, y = prop_win, fill = winner)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_text(aes(label=paste0(sprintf("%.3f", prop_win*100), "%"), y=prop_win/2), 
            position = position_dodge(width = 0.9),  
            colour="Black", 
            size = 2.5) + 
  labs(title = expression(paste("Outcome of Games between Teams 55 (", lambda[55], ": 0.03) vs. 75 (", lambda[75], ": 0.081)")), 
       subtitle = "Team 55: Controller, Fighter, Fighter, Mage, Marksman \nTeam 75: Controller, Fighter, Mage, Marksman, Slayer", 
       x = "",
       y = "Proportion",
       fill = "Outcome",
       caption = "Figure 7") + 
  scale_fill_discrete(name = "Outcome for \nTeam 55",  labels = c("Win",  "Lose")) +
  theme(plot.caption = element_text(hjust = 0.5, vjust = 4, size = 10),
        plot.subtitle = element_text(size = 8))
```


```{r}
actual_v2 <- matches %>% 
  filter((winner == "Fighter, Fighter, Mage, Mage, Marksman" & loser == "Fighter, Mage, Marksman, Marksman, Tank")| 
           (loser == "Fighter, Fighter, Mage, Mage, Marksman" & winner == "Fighter, Mage, Marksman, Marksman, Tank")) %>% 
  left_join(comps_df, by = c("winner" = "Team Composition")) %>% 
  group_by(winner, loser) %>% 
  mutate(n = n()) %>% 
  distinct() %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  mutate(prop_win = n / sum(n))  %>% #181 beats #232  wit hprob 0.666
  select(-loser) %>% 
  mutate(simulation = "Actual")

beta_sim2 <- 0.027 - 0.037 + 0.024 - 0.017 +  0.002 +  0.016
pi_sim2 <- exp(beta_sim2) / (1 + exp(beta_sim2))
set.seed(20200922)
simulation_v2 <- rbinom(100000, 1, pi_sim2) %>% 
  as_tibble() %>% 
  group_by(value) %>% 
  mutate(n = n()) %>% 
  distinct() %>% 
  mutate(`Comp ID` = ifelse(value == 1, 181, 232),
         winner = ifelse(value == 1, "Fighter, Fighter, Mage, Mage, Marksman", "Fighter, Mage, Marksman, Marksman, Tank"),
         simulation = "Simulated") %>% 
  arrange(desc(n)) %>% 
  ungroup() %>% 
  mutate(prop_win = n / sum(n)) %>% 
  select(winner, `Comp ID`, n, prop_win, simulation)
```

Figure 8 displays the actual and simulated outcomes of games between team composition 181 vs. 232. These two team compositions are not as commonly played - they only have 9 head to head matches. Out of these 9 games, team composition 181 won 6, or 66.667%, of its games. The estimated value of $\pi_{(181, 232)}$ is `r pi_sim2 %>% round(3)`. From 100,000 simulated games, team composition 181 won `r simulation_v2 %>% filter(winner == "Fighter, Fighter, Mage, Mage, Marksman") %>% pull(prop_win)%>% {.*100} %>% round(3)`% of its games. The proportions of the actual and simulated game outcomes are not similar for this situation - this is primarily due to the small number of actual head to head games between these two teams. This indicates that the model can struggle to properly estimate the strength ($\lambda_i$) of teams that are not commonly played. 

```{r sim-games-EDA2, fig.height=4, fig.align = 'center'}
bind_rows(
  actual_v2,
  simulation_v2
) %>% 
  ggplot(aes(x = simulation, y = prop_win, fill = winner)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_text(aes(label=paste0(sprintf("%.3f", prop_win*100), "%"), y=prop_win/2), 
            position = position_dodge(width = 0.9),  
            colour="Black", 
            size = 2.5) + 
  labs(title = expression(paste("Outcome of Games between Teams 181 (", lambda[181], ": 0.027) vs. 232 (", lambda[232], ": 0.037)")), 
       subtitle = "Team 181: Fighter, Fighter, Mage, Mage, Marksman \nTeam 232: Fighter, Mage, Marksman, Marksman, Tank", 
       x = "",
       y = "Proportion",
       fill = "Outcome",
       caption = "Figure 8") + 
  scale_fill_discrete(name = "Outcome for \nTeam 181",  labels = c("Win",  "Lose")) +
  theme(plot.caption = element_text(hjust = 0.5, size = 10),
        plot.subtitle = element_text(size = 8))
```



# Discussion  

## Selecting which Champion to Play

### First Pick

Now I will discuss some interesting applications of this ranking system. In this first application, we assume that you are the first player to pick a champion in a game. The ranking system can be used to determine which champion or champion class maximizes your team's expected composition strength.

Table 8 displays the strength values of the top 20 ranked champions. These strength values were calculated by averaging the $\lambda_i$'s obtained from (3) for each champion. For example, if champion Z was played on team compositions 1 and 2, champion Z's strength would be the average of the strengths of compositions 1 and 2.  

We see that the top 13 ranked champions are all Controllers - this reinforces the idea that balanced team compositions are generally better than ones with 5 offensive champions. In addition, the lowest ranked Controller (Ivern) is ranked 26th. However, Controllers are generally picked by the Support player, who is not always the first one to select their champion. The next best classes of champions are Slayers and Marksman - these two classes of champions can be played in almost any role^[[c]][Terminology]^ (Top, Jungle, Mid, Bottom or Support). Thus, regardless of the role of the first pick player, they have a wide pool of viable champions and 2-3 strong champion classes to pick from.

```{r}
champ_rank_tbl <- final_data %>% 
  left_join(
    bind_rows(
      tibble(
        comp = matches$winner %>% unique() ,
        comp_id = matches$winner %>% as.numeric() %>% unique() 
      ),
      tibble(
        comp = matches$loser %>% unique() ,
        comp_id = matches$loser %>% as.numeric() %>% unique() 
      )
    ) %>% 
      distinct() %>% arrange(comp_id) %>% 
      inner_join(
        tidybayes::gather_draws(ga_1_1_interactions, beta[i]) %>% 
          group_by(i) %>% 
          summarise(mean = mean(.value),
                    lower = quantile(.value, .025),
                    `50%` = quantile(.value, .5),
                    upper = quantile(.value, 0.975)),
        by = c("comp_id"="i")
      ),
    by = c("team_comp" = "comp")
  ) %>% 
  ungroup() %>% 
  select(team_comp, name, main_class, mean, lower, upper) %>% 
  ungroup() 
champ_rank_tbl %>% 
  group_by(name, main_class) %>%
  summarise(mean = mean(mean) %>% round(3),
            lower = mean(lower) %>% round(3),
            upper = mean(upper) %>% round(3)) %>%
  arrange(desc(mean)) %>%
  ungroup() %>%
  mutate(rownum = row_number(),
         `95% CI` = paste0("(", lower, ", ", upper, ")") %>% stringr::str_trim()) %>%
  select(rownum, name, main_class,  mean, `95% CI`) %>%
  rename(" " = rownum,
         "Champion Name" = name,
         "Class"  = main_class,
         "Strength" = mean) %>%
  slice(1:20) %>% 
  kableExtra::kable(caption = "Top 20 Champions, determined by averaging $\\lambda_i$'s from (3)",
        booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"))
```



```{r eval=FALSE}
display_ranking_betas(matches, ga_1_1_interactions, 1, 382) %>% 
  mutate(rownum = row_number()) %>% 
  mutate(class_in_comp = case_when(
    str_detect(Composition,  "Controller") ~ "Controller",
    str_detect(Composition,  "Marksman") ~ "Marksman",
    str_detect(Composition,  "Mage") ~ "Mage",
    str_detect(Composition,  "Tank") ~ "Tank",
    str_detect(Composition,  "Fighter") ~ "Fighter",
    str_detect(Composition,  "Specialist") ~ "Specialist",
    str_detect(Composition,  "Slayer") ~ "Slayer",
  )) %>% 
  group_by(class_in_comp) %>% 
  summarise(avg_ranking = mean(rownum)) %>% 
  arrange(avg_ranking)
filter(Composition %in% cc[str_detect(cc,  "Controller")]) %>% 
  summarise(avg = mean(rownum)) %>% 
  arrange(avg)

overall_rankings <- display_ranking_betas(matches, ga_1_1_interactions, 1, 382) %>% 
  mutate(rownum = row_number()) %>% 
  select(rownum, Composition, Strength)
bind_rows(
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Controller")]) %>% 
    mutate(class = "Controller") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Marksman")]) %>% 
    mutate(class = "Marksman") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Mage")]) %>% 
    mutate(class = "Mage") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Fighter")]) %>% 
    mutate(class = "Fighter") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Specialist")]) %>% 
    mutate(class = "Specialist") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Slayer")]) %>% 
    mutate(class = "Slayer") ,
  overall_rankings %>% 
    filter(Composition %in% cc[str_detect(cc,  "Tank")]) %>% 
    mutate(class = "Tank") 
) %>% 
  group_by(class) %>% 
  summarise(avg_ranking = mean(rownum)) %>% 
  arrange(avg_ranking)

overall_rankings %>% 
  filter(Composition %in% cc[str_detect(cc,  "Tank")]) %>% 
  summarise(avg = mean(rownum)) %>% 
  arrange(avg)

```

### Last Pick

```{r}
display_ranking_betas(matches, ga_1_1_interactions, 1, 382) %>% 
  separate(col = Composition, into = c("role1", "role2", "role3", "role4", "role5"), sep = ", ", remove = FALSE) %>% 
  filter((role1  == "Mage" | role2  == "Mage" | role3  == "Mage" | role4  == "Mage" | role5  == "Mage") & 
           (role1  == "Fighter" | role2  == "Fighter" | role3  == "Fighter" | role4  == "Fighter" | role5  == "Fighter") & 
           (role1  == "Tank" | role2  == "Tank" | role3  == "Tank" | role4  == "Tank" | role5  == "Tank") & 
           (role1  == "Slayer" | role2  == "Slayer" | role3  == "Slayer" | role4  == "Slayer" | role5  == "Slayer")) %>% 
  mutate(`Potential Last Pick` = c("Slayer", "Controller", "Marksman", "Specialist",  "Fighter", "Mage", "Tank"),
         `Overall Rank` = ` `) %>% 
  select(-(role1:role5)) %>% 
  select(`Overall Rank`,  Composition, `Potential Last Pick`, Strength, `95% CI`)  %>% 
  kableExtra::kable(caption = "Set of 4 Champion Classes: Fighter, Mage, Slayer and Tank",
        booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"), font_size = 10.5)
```

Now, we assume that you are the last player on your team to pick a champion. Once again, we want to determine which classes of champions would maximize your team's expected strength. For this application, I will pick two different sets of four champion classes, and determine the optimal last pick. The first set is Fighter, Mage, Slayer and Tank. Table 9 displays the rankings of the 7 different team compositions that contain this set of four champions. As last pick, the optimal champion class would be a Slayer - this combination of champion classes are ranked the highest, with a strength ($\lambda_i$) value of 0.071. The next best champion class is a Controller, which is not surprising. From Table 8, we see that Controllers are some of the highest ranked champions. One reason why a last pick Controller may not be the best option is that the team already has a defensive champion (a Tank). The lowest ranked last pick is a Tank - this suggests that the player picking last should not pick a Tank. This could be due to the lack of offensive power with two defensive and low damage champions or the model's struggles to distinguish between team compositions due to many team compositions having an approximately 50% win rate. 

```{r}
display_ranking_betas(matches, ga_1_1_interactions, 1, 382) %>% 
  separate(col = Composition, into = c("role1", "role2", "role3", "role4", "role5"), sep = ", ", remove = FALSE) %>% 
  filter((role1  == "Controller" | role2  == "Controller" | role3  == "Controller" | role4  == "Controller" | role5  == "Controller") & 
           (role1  == "Fighter" | role2  == "Fighter" | role3  == "Fighter" | role4  == "Fighter" | role5  == "Fighter") & 
           (role1  == "Marksman" | role2  == "Marksman" | role3  == "Marksman" | role4  == "Marksman" | role5  == "Marksman") & 
           (role1  == "Slayer" | role2  == "Slayer" | role3  == "Slayer" | role4  == "Slayer" | role5  == "Slayer")) %>% 
  mutate(`Potential Last Pick` = c("Specialist", "Fighter", "Controller", "Mage",  "Tank", "Slayer", "Marksman"),
         `Overall Rank` = ` `) %>% 
  select(-(role1:role5)) %>% 
  select(`Overall Rank`,  Composition, `Potential Last Pick`, Strength, `95% CI`)  %>% 
  kableExtra::kable(caption = "Set of 4 Champion Classes: Controller, Fighter, Marksman and Slayer",
        booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"), font_size = 10.5) 
```

The second set of four champions is Controller, Fighter, Marksman and Slayer. Table 10 displays the rankings of the 7 different team compositions that contain this set of four champions. As last pick, the best two classes to select would be a Specialist and Fighter - these also correspond with the top 2 ranked team compositions. A last pick Controller or Mage also creates strong teams, but their strength ($\lambda_i$) are slightly lower than that of the top 2 picks. The worst last pick, a Marksman, is ranked 322nd overall. This could be because double Marksman compositions are hard to execute, and solo-queue players may not have enough synergy or communication to execute this type of composition. 

It is worth noting the differences between the optimal first and last picks. For example, Tanks are the weakest first pick but move up to the 5th best last pick. This can be deceiving, but note that the team composition with a last pick Tank is ranked 10th overall. In addition, Marksman are a relatively strong first pick, but seem to perform very poorly in a team composition when it is the last  pick. 

# Conclusion & Limitations

This paper utilized a hierarchical Bayesian logistic regression model (a reduction of the Bradley-Terry model) to form a team composition ranking system. It also provides an insight into which player-level statistics are the most significant features when determining a team composition's overall rank.

The major limitation from this analysis is the lack of data for certain team compositions. There are a handful of team compositions that are recorded only once or twice - these compositions end up having eiter 0% or 100% win rates. However, this is highly unlikely in solo-queue. In addition, with the majority of teams having an approximately 50% win rate, the model struggles to distinguish between team compositions. This could be a reason why the strength ($\lambda_i$) values are relatively small and close to each other. 

Further work includes examining other formations of team compositions (potentially by champion name instead of champion class) and the inclusion of more player-level variables. 

# Appendix 

## References  

\noindent
1). Bradley, R., and Terry, M. (1952). Rank analysis of incomplete block desings: The method of paired comparisons.
Biometrika 39, 324–345.

\noindent
2). Caron, F., & Doucet, A. (2012). Efficient Bayesian Inference for Generalized Bradley–Terry Models. Journal of Computational and Graphical Statistics, 21(1), 174-196. doi:10.1080/10618600.2012.638220 [http://www.stats.ox.ac.uk/~doucet/caron_doucet_bayesianbradleyterry.pdf](http://www.stats.ox.ac.uk/~doucet/caron_doucet_bayesianbradleyterry.pdf)

\noindent
3). Phelan, G. C., & Whelan, J. T. (2018). Hierarchical Bayesian Bradley–Terry for applications in Major League Baseball. Mathematics for Application, 7(1), 71-84. doi:10.13164/ma.2018.07 [https://arxiv.org/pdf/1712.05879.pdf](https://arxiv.org/pdf/1712.05879.pdf)

\noindent
4). Kang D., and Kim M. (2015). Poisson Model and Bradley terry Model for predicting multiplayer online battle games. Seventh International Conference on Ubiquitous and Future Networks, Sapporo, 2015, pp. 882-887, doi: 10.1109/ICUFN.2015.7182671. [https://ieeexplore.ieee.org/document/7182671](https://ieeexplore.ieee.org/document/7182671)

\noindent
5). League of Legends(LOL) - Ranked Games 2020, [https://www.kaggle.com/gyejr95/league-of-legendslol-ranked-games-2020-ver1](https://www.kaggle.com/gyejr95/league-of-legendslol-ranked-games-2020-ver1)

\noindent
6). Riot Developer Portal, [https://developer.riotgames.com/](https://developer.riotgames.com/).

\noindent
7). Hoff, P. D. (2010). Chapter 1: Introduction and examples. In A first course in bayesian statistical methods (pp. 5-7). Dordrecht: Springer.

\noindent
8). Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2019). Pareto smoothed importance sampling. arXiv preprint arXiv:1507.04544.

## Terminology 

\noindent
a). **Class** - One of seven classes assigned to champions that play similar styles. The five offensive classes are Fighters, Mages, Marksman, Slayers and Specialists. The two defensive classes are Controllers and Tanks. 

\noindent
b).  **Solo-queue** - Whenever a person or a group of people queue up for the game. Solo-queue generally refers to ranked games. Playing ranked games will determine a player's Elo, which will rise and fall based on their overall win ratio.

\noindent
c). **Champion Role** - Top, Jungle, Middle, Bottom, or Support, where the Bottom and Support play in the Bottom lane. Players will indicate their preferred role before the match-making system puts them in a game.  

## Inconsistencies with Data 

Teams consist of 5 players, each with a unique role - Top, Jungle, Mid, Bot, and Support. There are games where players' roles were either missing or recorded as *none*. In addition, Bot laners were initially labeled as one of three categories: *duo\_carry*, *duo\_support* and *duo*. The first two refer to the Bot and Support roles respectively, while *duo* does not clearly differentiate between Bot and Support. I decided not to impute any missing or inconsistent data. Since champions can hypothetically be played in any role, imputation may not take that into consideration and could bring unwanted variability to the results. Thus, I removed games where any player's role is missing or recorded as *none* or *duo*. This process results in the final dataset, which contains 31,909 games.

## Model Diagnostics 

```{r}
beta_diagnostics <- tidybayes::gather_draws(ga_1_1_interactions, beta[i])

trace1 <- beta_diagnostics %>% 
  filter(i %in% c(89,60,264,18)) %>%
  mutate(param = ifelse(is.na(i), .variable, paste0("lambda","[", i, "]"))) %>%   
  mutate(param = factor(param,
                        levels = c("lambda[89]", "lambda[60]", "lambda[264]", "lambda[18]"),
                        labels = c(expression(lambda[89]),
                                   expression(lambda[60]),
                                   expression(lambda[264]),
                                   expression(lambda[18])))) %>% 
  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) + 
  geom_line(alpha = 0.6) + 
  labs(title = expression(paste("Traceplots for ", lambda[i])),
       subtitle = "from Team Compositions 89, 60, 264 and 18",
       x = "Iteration", 
       y = "Value", color = "Chain",
       caption = "Figure 4") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 16),
        plot.title = element_text(size = 18)) +
  facet_wrap(param~.,scales = "free_y",labeller = "label_parsed") 

trace2 <- beta_diagnostics %>% 
  filter(i > 382) %>%
  mutate(param = ifelse(is.na(i), .variable, paste0("beta","[", i-382, "]"))) %>% 
  mutate(param = factor(param,
                        levels = c("beta[1]", "beta[2]","beta[3]","beta[4]"),
                        labels = c(expression(beta[1]),
                                   expression(beta[2]),
                                   expression(beta[3]),
                                   expression(beta[4])))) %>% 
  ggplot(aes(x = .iteration, y = .value, color = as.factor(.chain))) + 
  geom_line(alpha = 0.6) + 
  labs(title = expression(paste("Traceplots for ", beta[p])),
       x = "Iteration", 
       y = "Value", color = "Chain",
       caption = "Figure 5") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 16),
        plot.title = element_text(size = 18)) +
  facet_wrap(param~., scales = "free_y", labeller = "label_parsed")
```

```{r fig.align="center"}
trace1
```

```{r fig.align="center"}
trace2
```

```{r}
(summary(ga_1_1_interactions))$summary[, "Rhat"] %>% as_tibble() %>% 
  summarise(Min = min(value), 
            `2.5%` = quantile(value, 0.025),
            `25%` = quantile(value, 0.25),
            `50%` = quantile(value, 0.5),
            `75%` = quantile(value, 0.75),
            `97.5%` = quantile(value, 0.975),
            Max = max(value) 
  ) %>% 
  mutate_if(is.double, round, 3) %>% 
  kableExtra::kable(caption = "$\\hat{R}$ values for $\\lambda_i$, $\\beta_p$, $\\sigma^2_{comp}$ from (3)", 
                    booktabs = TRUE, align = "c") %>% 
  kableExtra::kable_styling(latex_options = c("hold_position"))

```

## Full Team Composition Rankings

```{r display full rank}
display_ranking_betas(matches, ga_1_1_interactions, 1, 45) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
  kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 46, 90) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 91, 135) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 136, 180) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 181, 225) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 226, 270) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 271, 315) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 316, 360) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
display_ranking_betas(matches, ga_1_1_interactions, 361, 382) %>% 
  kableExtra::kable(caption = "Team Compositions Ranked by Strength, $\\hat{\\lambda}_i$, from (3)", 
                    booktabs = TRUE, align = "c")  %>% 
    kableExtra::kable_styling(font_size = 10)
```

```{r fig.align="center", fig.height=4.3, eval=FALSE}
## Sensitivity Analysis 

#In Figure 6 in the Apendix, I explore the posterior distributions of $\lambda_{89}$ across different priors on $\sigma^2_{comp}$. The blue lines indicate the posterior mean of $\lambda_{89}$, `r post_mean_lambda89 %>% round(3)`, estimated from (3). The posterior distributions appear to be roughly consistent across different priors. 

sen_analysis_priors_names <- c("Ga(1, 1))",
                               "Ga(0.1, 1)",
                               "Ga(2, 1))",
                               "Ga(1, 1) no int")   
sen_analysis_priors <- list(ga_1_1_interactions,
                            ga_0.1_1_interactions,
                            ga_2_1_interactions,
                            ga_1_1)

sen_tbl_list <- list()
for (model in 1:4) {
  sen_tbl_list[[model]] <- tidybayes::gather_draws(sen_analysis_priors[[model]], beta[i]) %>% 
    ungroup() %>% 
    filter(i == 89) %>% 
    mutate(model = sen_analysis_priors_names[model])
}

post_mean_lambda89 <- tidybayes::gather_draws(ga_1_1_interactions, beta[i]) %>% 
  ungroup() %>% 
  filter(i == 89) %>% 
  summarise(mean = mean(.value)) %>% pull(mean)

do.call(bind_rows, sen_tbl_list) %>% 
  dplyr::mutate(model = factor(model,
                               levels = sen_analysis_priors_names,
                               labels = c(expression(paste(sigma[comp]^2, " ~ Ga(1, 1)")),
                                          expression(paste(sigma[comp]^2, " ~ Ga(0.1, 1)")),
                                          expression(paste(sigma[comp]^2, " ~ Ga(2, 1)")),
                                          expression(paste(sigma[comp]^2, " ~ Ga(1, 1),  no interaction"))))) %>%
  ggplot() + 
  geom_density(aes(x = .value)) + 
  geom_vline(xintercept = post_mean_lambda89, color = "blue") + 
  facet_wrap(model~., labeller = "label_parsed") + 
  labs(title = expression(paste("Posterior Distributions for ", lambda[89])),
       x = "Value",
       y = "Density",
       caption = "Figure 6") +
  theme(plot.caption = element_text(hjust = 0.5, vjust = -0.5, size = 12))
```

